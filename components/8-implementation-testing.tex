\chapter{Testing Implementation}
\game{} was tested continuously throughout development. This was primarily done using a single machine with multiple instances of the game running as separate processes, each of which communicated via a single NFD. This was an ideal setup for design, development and debugging. However, it is not representative of a real scenario in which multiple players are playing \game{}. As such, a more realistic approach was required. 

A description of the libraries and frameworks used, and the software developed to facilitate real world testing of \game{} is provided in the following sections.



\section{Player Automation}\label{sec:impl:automation}
In order to enable testing without actual people playing the game, a simple automation mechanism was developed. This was accomplished by creating a \textit{InputController} interface. Two implementations of this interface were created - a \textit{RealInputController} which actually reacts to user input via the keyboard and mouse, and an \textit{AutomatedInputController} which simulates key strokes.

The automation script used caused players to move in approximate paths, while shooting projectiles approximately once per second and placing blocks approximately once every 10 seconds. A random number generator was used to ensure that a player would not just repeatedly loop through the same move-set. However, a fixed seed was used for the random number generator, providing repeatability across tests using different topologies and parameters, enabling direct comparisons between these tests. 


\section{Docker}\label{sec:impl:test:docker}
As previously discussed, having several Java processes communicating using a single NFD does not represent a realistic scenario. Thus, an approach using a separate NFD for each player was developed, which closely resembles the real world use case of multiple players playing on different machines connected by a network.

To accomplish this, Docker was used. Docker allows for the creation of entirely self contained \textit{containers}, which represent a \textit{standardized unit of software} \cite{docker}. Docker containers run on a Docker engine, which is built for a specific host operating system such as Linux. However, the Docker containers themselves are entirely independent to the host operating system, meaning they run identically, regardless of the underlying platform.

A Docker \textit{image} is used to define a Docker container. This contains \textbf{everything} that is needed to run the piece of software, such as libraries, frameworks and dependencies. A Docker image which contains everything required to run \game{} was developed based on Peter Gusev's \cite{docker-ndn} NDN Docker image. This enabled \game{} to be run on any machine which had Docker installed.

Docker Compose \cite{docker-compose} is a tool which allows for the deployment of multiple Docker containers at once. This is done by defining Docker \textit{Services} inside of a \textit{compose file}. Each \textit{service} definition in a Docker \textit{compose file} contains the information for launching a number of containers such as what image to use and how many replicas of the image are required. For each of the topologies tested, a separate \textit{compose file} was created, which contained a \textit{service} for each node in the topology. Thus, each automated game player ran as a \textit{service}, as did each intermediate router.

Docker also allows for the creation of virtual networks, over which Docker containers can communicate. The default network type is a \textit{bridge} network, which enables containers to communicate with one and other, provided they are on the same host machine. This allowed the NFDs of each of the nodes (game players and intermediate routers) to communicate with one and other through the bridge network. 

This was accomplished by giving each of the nodes a \textit{network alias}. For example, node A was given the alias \textit{nodea.ndngame.com} and node B was given the alias \textit{nodeb.ndngame.com} etc. Docker Compose supports defining network aliases in the service definition, and this is the reason each node was defined as a separate service.

At this point, a single Docker Compose command could create all of the \game{} nodes required by the topology, as defined in the compose file, and run them on the host machine. In order to test the scalability of \game{}, dozens of containers needed to be run simultaneously. However, this approach did not scale well on a single machine, and an approach was required to enable deploying the Docker containers across a cluster.

Docker supports orchestrating clusters of separate Docker hosts using \textit{swarms}. The services contained in a docker compose file can be deployed across a swarm using a Docker \textit{stack}. The final requirement was to enable the Docker containers running on separate hosts to communicate with one and other. As discussed previously, \textit{bridge} networks only allow containers running on the same \textbf{Docker host} to communicate.

Docker allows containers running on different Docker hosts to communicate by using an \textit{overlay} network. The actual network is entirely managed by Docker and allowed all of the \game{} nodes to communicate regardless of what Docker host their container resided on. 

When using swarms, Docker creates and uses an \textit{ingress routing mesh} by default. This allows requests which reach any node in the swarm to be routed to an appropriate node who can service the request. For example, if a stack consists of a database container which exposes port 3306, and a web server container which exposes port 80, if a request for a web page (using port 80) is made to the database container, the ingress routing mesh automatically forwards this requests to the web server container. This is an extremely useful feature of Docker as it allows requests to be serviced regardless of the actual node the request is sent to. Similarly, this can also be used for load balancing requests across the swarm.

However, this behaviour is not desired in the case of testing \game{}, as Interests that are forwarded to a particular network alias, \textbf{must} arrive at the specific node they are forwarded to. Each of the nodes exposed the default NFD port of 6363, meaning Interests could actually arrive at different node to the one it was meant to be forwarded to. To overcome this problem, the endpoint mode of each Docker service required by the topology was set to \textit{DNS-round-robin}, instead of the default \textit{ingress routing mesh}.

An example topology containing 2 game players, and the corresponding Docker compose file used to test the topology are shown in \refapp{app:docker-two-players}.



\section{NLSR Configuration}\label{sec:impl:topologies}
As defined in the previous section, each of the nodes used to test \game{} can communicate using a UDP tunnel through the overlay network created by Docker. However, a key component of the testing is defining the NDN topology and seeing how it impacts the performance of \game{}. Even though every node can directly access every other node using a UDP tunnel, the actual paths taken by the NDN Interest / Data packets are defined by the FIB of the NFDs of each node. 

The Docker overlay network serves as a means to create the communication links required by the NDN topology. This aligns with the vision of NDN being the \textit{narrow waist of tomorrow's Internet}, with UDP/IP being one of the lower level protocols that NDN can in turn use for communication.

As described in \refsec{sec:NLSR}, NLSR is the routing protocol used by NDN to discover nearby nodes (routers) and to build the FIB. The NLSR daemon requires a configuration file which defines a variety of NLSR specific parameters, as well as the name of the node, adjacent nodes and the name prefixes which this node can produce data under. 

By creating a configuration file for each node in the NDN topology, and running the NLSR daemon on each of the test nodes with the appropriate configuration file, the NDN topology could be fully defined, and all players will discover the required prefixes and their NFDs will function appropriately.

To examine the impacts of the topology on the performance of \game{}, a Python script was developed to perform the following:

\begin{enumerate}
    \item Allow for topologies to be defined in code, supporting both game players and intermediate routers.
    \item Generate appropriate NLSR config files for each node in the topology
    \item Generate an appropriate Docker Compose file for the topology, allowing all nodes in the topology to communicate and for the entire test to be runnable with a single command.
\end{enumerate}

The script was then used to generate the required files for testing a variety of topologies such as the ones shown below:

\begin{figure}[H]
    \centering
    \figsize{assets/impl/topologies.png}{0.8}
    \caption{An example of the topologies used for testing \game{}}
    \label{fig:impl:topologies}
\end{figure}

These topologies were chosen as they offer a good coverage of possible topologies. The \textit{linear} and \textit{square} topologies represent two and four player nodes respectively without any intermediate routers. The \textit{tree} and \textit{dumbbell} topologies model hierarchical topologies, such as what would be found with nodes A and B connected to one Internet service provider (ISP), and nodes C and D connected to another.  

\section{Metrics}\label{sec:impl:metrics}
In order to examine the performance of \game{}, several metrics were required. There were two sources of data for these metrics: the \game{} processes running on the player nodes and the NFD logs running on the player nodes and intermediate router nodes. 

To gather data from the \game{} Java processes, Dropwizard's Metrics framework \cite{dropwizard-metrics} was used. This provides many useful classes for gathering data such as histograms, counters, timers and rate-trackers. All of the data tracked by this framework can be monitored in real-time using a web dashboard. However, to perform a more detailed analysis, the data was also periodically written to a CSV file.


Gathering these metrics proved to be considerably more challenging than expected due to nature of NDN. The main issue is that Interest packets are aggregated and Data packets are multicasted to consumers. This means there is no way for producers to know which consumer they are serving. Similarly, consumers cannot know if they are obtaining a cached copy of a piece of Data, or if the piece of Data they receive is the result of another player expressing the same Interest at an earlier point in time. 

\subsection{Round-Trip-Time (RTT)}
One of the main challenges was attempting to understand the temporal performance of the network. Recall that the sync protocol uses a long lived outstanding Interest model, and that producers only reply when there is an update worth sending. Thus if a consumer expresses an Interest at time $t_{int}$, and receives a Data packet at time $t_{data}$, the RTT can be calculated using \refeq{eq:impl:rtt},

\begin{equation}\label{eq:impl:rtt}
    RTT = t_{data} - t_{int}
\end{equation}

Consider the case where a consumer calculates a RTT of 500ms. This would suggest that the network is performing very poorly. However, this may be due to the fact that the producer had no updates to send when the Interest arrived,  meaning the Interest took considerably longer to satisfy, even though the network may be performing well.

The obvious solution to this problem is to have producers respond with the amount of time that elapsed between receipt of the Interest, and an update becoming available, $t_{wait}$. This would allow the RTT calculation to be adjusted, yielding the \textit{effective RTT, $RTT_{eff}$} as shown in \refeq{eq:impl:rtt-eff}.

\begin{equation}\label{eq:impl:rtt-eff}
    RTT_{eff} = (t_{data} - t_{int}) - t_{wait}
\end{equation}

However, due to Interest aggregation, the producer will only see one Interest for a given Data name, regardless of the number of consumers who express the same Interest. Thus, the producer can only measure $t_{wait}$ for the first Interest they receive. Similarly, the Data packet received by all of the consumers will be identical due to the native multicast feature of NDN. Thus, $t_{wait}$ will only be correct for the consumer who's Interest reached the producer first.

This problem does not exist in a typical IP implementation as the producer would receive a request from each consumer and thus be able to calculate $t_{wait}$ accordingly.

For this reason, the first RTT metric $RTT$ was used instead of $RTT_{eff}$ and this effect was taken into account when evaluating RTTs.


\subsection{Interest Aggregation Factor (IAF)}
As previously discussed, one of the major benefits of NDN in a P2P MOG context such as \game{} is Interest aggregation. The \textit{interest aggregation factor (IAF)} is a measure of how much Interest aggregation is occurring in the network. The IAF of a particular producer is the ratio of the number of Interests seen by the producer to the number of Interests expressed by consumers for Data owned by that producer.

The \textit{IAF} for a player $p$, $iaf_p$, can be calculated using \refeq{eq:impl:iaf}, where $P$ represents the set of all players, $n_{seen\_p}$ represents the number of Interests seen by player $p$ and $n_{exp\_x\_p}$ represents the number of Interests expressed by player \textit{x} for Data produced by player \textit{p}.

\begin{equation}\label{eq:impl:iaf}
    iaf_p = \frac{n_{seen\_p}}{\sum\limits_{x \epsilon P}^{}{n_{exp\_x\_p}}}
\end{equation}


\subsection{Cache Hit Rate}
As there is no way for consumers to know if they received a cached copy of a Data packet, the easiest way to accurately determine cache rates was to enable debug mode logging in the NFD's \textit{ContentStore} module. This produces log messages indicating whether or not an entry existed in a CS. A Python script was then written to parse the log files of all the participating nodes to determine their cache hit rates.

A more elegant alternative would be to add this functionality to the NFD used by all of the nodes. However, the amount of log messages produced by enabling logging of the \textit{ContentStore} module was sufficiently small that the simpler approach sufficed.

\subsection{Position Deltas}
In order to investigate the performance of the dead reckoning (DR) and Interest management (IM) features, a metric was required which captured the error associated with enabling these features and adjusting their parameters.

The \textit{position delta} metric is calculated on receipt of an update for a game object. It is simply the Euclidean distance between the position of the local copy of the remote game object, and the position contained in the received update. 

The network benefits of DR and IM can be examined using other metrics, but the more aggressively these systems are used, the worse the gameplay experience becomes. However, as the test players are automated and run in headless mode, the \textit{position delta} was used to attempt to numerically measure the associated impact on gameplay experience.


