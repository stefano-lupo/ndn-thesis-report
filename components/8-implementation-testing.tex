\chapter{Testing Implementation}
\game{} was tested continuously throughout development. This was primarily done using a single machine with multiple instances of the game running as separate processes, each of which communicated via a single NFD. This was an ideal setup for design, development and debugging. However, it is not representative of a real scenario in which multiple players are playing \game{}. As such, a more realistic approach was required. The testing procedure used to investigate the performance of \game{} is outlined in the following sections. \todo[]{reference INCREDIBLES}



\section{Player Automation}\label{sec:impl:automation}
Must be repeatable, used fixed seed for RNG
In order to enable testing without actual people playing the game, a simple automation mechanism was developed. This was accomplished by creating a \textit{InputController} interface. Two implementations of this interface were created - a \textit{RealInputController} which actually reacts to user input via the keyboard and mouse, and an \textit{AutomatedInputController} which simulates key strokes.

The automation script used caused players to move in approximate paths, while shooting projectiles approximately once per second and placing blocks approximately once every 10 seconds. A random number generator was used to ensure that a player would not just repeatedly loop through the same moveset. However, a fixed seed was used for the random number generator, providing repeatability across tests using different topologies and parameters and enabling direct comparisons. 


\section{Docker}\label{sec:impl:test:docker}
As previously discussed, having several Java processes communicating using a single NFD does not represent a realistic scenario and would produce data which does not reflect reality. Thus, an approach using a separate NFD for each player was developed, which closely resembles the real world use case of multiple players playing on different machines connected by a network.

To accomplish this, Docker was used. Docker allows for the creation of entirely self contained \textit{containers}, which represent a \textit{standardrized unit of software} \cite{docker}. Docker containers run on a Docker engine, which is built for a specific host operating system such as Linux. However, the Docker containers themselves are entirely independent to the host operating system, meaning they run identicially, regardless of the underlying platform.

A Docker \textit{image} is used to define a Docker container. This contains \textbf{everything} that is needed to run the piece of software, such as libraries, frameworks and dependencies. A Docker image which contains everything required to run \game{} was developed based on Peter Gusev's \cite{docker-ndn}. This enabled \game{} to be run on any machine which had Docker installed which was ideal for testing.

Docker Compose \cite{docker-compose} is a tool which allows for the deployment of multiple Docker containers at once. This is done by defining Docker \textit{Services} inside of a \textit{compose file}. Each \textit{service} definition in a Docker \textit{compose file} contains the information for launching a number of containers such as what image to use and how many replicas of the image are required. For each of the topologies tested, a separate \textit{compose file} was created, which contained a \textit{service} for each node in the topology. Thus, each automated game player ran as a \textit{service}, as did each intermediate router.

Docker also allows for the creation of virtual networks, over which Docker containers can communicate. The default network type is a \textit{bridge} network, which enables containers to communicate with one and other, provided they are on the same host machine. This allowed the NFDs of each of the nodes (game players and intermediate routers) to communicate with one and other through the bridge network. 

This was accomplished by giving each of the nodes an \textit{network alias}. For example, node A was given the alias \textit{nodea.ndngame.com} and node B was given the alias \textit{nodeb.ndngame.com} etc. Docker Compose supports defining network aliases in the service definition and this is the reason each node was given a separate service.

At this point, a single Docker Compose command could create all of the \game{} nodes required by the topology, as defined in the compose file, and run them on the host machine. In order to test the scalability of \game{}, dozens of containers needed to be run simultaneously. However, this approach did not scale well and an approach was required to enable deploying the Docker containers across a cluster.

Docker supports orchestrating clusters of separate Docker hosts using \textit{swarms}. The services contained in a docker compose file can be deployed across a swarm using a Docker \textit{stack}. The final requirement was to enable the Docker containers created as a result the deployment to communicate with one and other. As discussed previously, \textit{bridge} networks only allow containers running on the same \textbf{Docker host} to communicate.

Docker allows containers running on different Docker hosts to communicate by using an \textit{overlay} network. The actual network is entriely managed by Docker and allowed all of the \game{} nodes to communicate regardless of what Docker host their container resided on. 

When using swarms, Docker creates and uses an \textit{ingress routing mesh} by default. This allows requests which reach any node in the swarm to be routed to an appropriate node who can service the request. For example, if a stack consists of a database container which exposes port 3306, and a web server container which exposes port 80, if a request for a web page (using port 80) is made to the database container, the ingress routing mesh automatically forwards this requests to the web server container. This is an extremelly useful feature of Docker as it allows requests to be serviced regardless of the actual node the request is sent to. Similarly, this can also be used for load balancing requests across the swarm.

However, this behaviour is not desired in the case of testing \game{}, as Interests \textbf{must} arrive at the node they are forwarded. Each of the nodes use the default NFD port of 6363, meaning Interests could actually arrive at different node to the one it was meant to be forwarded to. This required the use of \textit{DNS-round-robin} endpoint mode instead of the default \textit{ingress routing mesh}, which is specifiable at the service level in a compose file.

An example topology containing 2 game players, and the corresponding Docker compose file used to test the topology are shown in \refapp{app:docker-two-players}.



\section{NLSR Configuration}\label{sec:impl:topologies}
As defined in the previous section, each of the nodes used to test \game{} can communicate using a UDP tunnel through the overlay network created by Docker. However, a key component of the testing is defining the NDN topology and seeing how it impacts the performance of \game{}. Even though every node can directly access every other node using a UDP tunnel, the actual paths taken by the NDN Interest / Data packets are defined by the FIB of the NFDs of each node. 

The Docker overlay network serves as a means to create the communication links required by the NDN topology. This aligns with the vision of NDN being the \textit{narrow waist of tomorrow's Internet}, with UDP/IP being one of the lower level protocols that NDN can in turn use for communication.

As described in \refsec{sec:NLSR}, NLSR is the routing protocol used by NDN to discover nearby nodes (routers) and to build the FIB. The NLSR daemon requires a configuration file which defines a variety of NLSR specific parameters, as well as the name of the node, adjacent nodes and the name prefixes which this node can produce data under. 

By creating a config file for each node in the NDN topology and running the NLSR daemon on each of the test nodes using the appropriate config file for that node, all players will discover the required prefixes and their NFDs will function appropriately.

To examine the impacts of the topology on the performance of \game{}, a Python script was developed to perform the following:

\begin{enumerate}
    \item Allow for topologies to be defined in code, supporting both game players and intermediate routers.
    \item Generate appropriate NLSR config files for each node in the topology
    \item Generate an appropriate Docker Compose file for the topology, allowing all nodes in the topology to communicate and for the entire test to be runnable with a single command.
\end{enumerate}

The script was then used to generate the required files for testing a variety of topologies such as the ones shown below:

\begin{figure}[H]
    \centering
    \figsize{assets/impl/topologies.png}{0.8}
    \caption{An example of the topologies used for testing \game{}}
    \label{fig:impl:topologies}
\end{figure}


\section{Metrics}\label{sec:impl:metrics}
In order to examine the performance of \game{}, several metrics were required. There were two sources of data for these metrics: the \game{} processes running on the player nodes and the NFD logs running on the player nodes and intermediate router nodes. 

To gather data from the \game{} Java processes, Dropwizard's Metrics framework \cite{dropwizard-metrics} was used. This provides many useful classes for gathering data such as histograms, counters, timers and rate-trackers. All of the data tracked by this framework can be monitored in real-time using a web dashboard. However, to perform a more detailed analysis, the data was also periodically written to a CSV file.


Gathering these metrics proved to be considerably more challenging than expected due to nature of NDN. The main issue is that Interest packets are aggregated and Data packets are multicasted to consumers. This means there is no way for producers to know which consumer they are serving. Similarly, consumers cannot know if they are obtaining a cached copy of a piece of Data, or if the piece of Data they receive is the result of another player expressing the same Interest at an earlier point in time. 

\subsection{Round-Trip-Time (RTT)}
One of the main challenges was attempting to understand the temporal performance of the network. Recall that the sync protocol uses an long lived outstanding Interest model and that producers only reply when there is an update worth sending. Thus if a consumer expresses an Interest at time $t_{int}$, and receives a Data packet at time $t_{data}$, the RTT can be calculated using \refeq{eq:impl:rtt},

\begin{equation}\label{eq:impl:rtt}
    RTT = t_{data} - t_{int}
\end{equation}

Consider the case where a consumer calculates a RTT of 500ms. This would suggest that the network is performing very poorly. However, this may be due to the fact that the producer had no updates to send when the Interest arrived,  meaning the Interest took considerably longer to satisfy, even though the network may be performing well.

The obvious solution to this problem is to have producers respond with the amount of time that ellapsed between receipt of the Interest, and an update becoming available, $t_{wait}$. This would allow the RTT calculation to be adjusted, yielding the \textit{effective RTT, $RTT_{eff}$} as shown in \refeq{eq:impl:rtt-eff}.

\begin{equation}\label{eq:impl:rtt-eff}
    RTT_{eff} = (t_{data} - t_{int}) - t_{wait}
\end{equation}

However, due to Interest aggregation, the producer will only see one Interest regardless of the number of consumers. Thus, the producer can only measure $t_{wait}$ for the first Interest they receive. Similarly, the Data packet received by all of the consumers will be identical due to the native multicast feature of NDN. Thus, $t_{wait}$ will only be correct for the consumer who's Interest reached the producer first.

This problem does not exist in a typical IP implementation as the producer would receive a request from each consumer and thus be able to calculate $t_{wait}$ accordingly. 

\todo[]{I forget what happens with the latency calculation as data received timestamp - timestamp in data name}

\subsection{Interest Aggregation Factor (IAF)}
As previously discussed, one of the major benefits of NDN in a P2P MOG context such as \game{} is Interest aggregation. The \textit{interest aggregation factor (iaf)} is a measure of how many of the Interests produced by consumers are aggregated at intermediate hops. The \textit{iaf} for a player $p$, $iaf_p$, can be calculated using \refeq{eq:impl:iaf}, where $P$ represents the set of all players, and $n_{seen\_p}$ represents the number of Interests seen by player $p$ and $n_{exp\_x\_p}$ represents the number of Interests expressed by player \textit{x} for Data produced by player \textit{p}.

\begin{equation}\label{eq:impl:iaf}
    iaf_p = \frac{n_{seen\_p}}{\sum\limits_{x \epsilon P}^{}{n_{exp\_x\_p}}}
\end{equation}

\subsection{Native Multicast Factor}
\todo[]{Once I see if this is doable}

\subsection{Cache Hit Rate}
As there is no way for consumers to know if they received a cached copy of a Data packet, the easiest way to accurately determine cache rates was to enable debug mode logging in the NFD's \textit{ContentStore} module. This produces log messages indicating whether or not an entry existed in a CS. A Python script was then written to parse the log files of all the participating nodes to determine their cache hit rates.

\todo[]{Cache rates by node and by num interests?}

A more elegant alternative would be to add this functionality to the NFD used by all of the nodes. However, the amount of log messages produced by enabling logging of the \textit{ContentStore} module was sufficiently small that the simpler approach sufficed.

\subsection{Position Deltas}
In order to investigate the performance of the dead reckoning (DR) and Interest management (IM) features, a metric was required which captured the error associated with enabling these features and adjusting their values.

The \textit{position delta} metric is calculated on receipt of an update for a game object. It is simply the Euclidean distance between where the local copy of the game object is and the updated position contained in the received update. 

The network benefits of DR and IM can be examined using other metrics, but the more aggresively these systems are used, the worse the gameplay experience becomes. However, as the test players are automated and run in headless mode, the \textit{position delta} was used to attempt to numerically measure the associated impact on gameplay experience.


