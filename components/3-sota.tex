\chapter{State of the Art (15 to 25)}

\section{Named Data Networking}\label{sec:ndn-sota}
Today, most networks make use of the so called Internet Protocol (IP) as the primary mechanism for global communication. The design of IP was heavily influenced by the success of the 20th century telephone networks, resulting in a protocol tailored towards point-to-point communication between two hosts. IP is the \textit{universal network layer} of today's Internet, which implements the minimum functionality required for global interconnectivity. This represents the so called \textit{thin waist} of the Internet, upon which many of the vital systems in use today are built \cite{ndn-exec-summary}. The design of IP was paramount in the success of the modern day internet. However, in recent years, the Internet has become used in a variety of new non point-to-point contexts, rendering the inherent host based abstraction of IP less than ideal.  

The Named Data Networking project is a continuation of an earlier project known as Content-Centric Networking (CCN) \cite{vj-named-content}. The CCN and NDN projects represent a shift in how networks are designed, from the host-centric approach of IP to a data centric approach. NDN provides an alternative to IP, maintaining many of they key features which made it so successful, while improving on the shortcomings uncovered after three decades of use. The design of NDN aligns with the \textit{thin waist} ideology of today's Internet and NDN strives to be the universal network layer of tomorrow's Internet. 

% Typically, today's applications are written in terms of what information they want, rather than where it is located, requiring application-specific middleware to map between the application's data model and the Internet's data model \cite{ndn-project}. As the Internet has evolved, it has become a primary channel for content distribution

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Primitives
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NDN Primitives}
In NDN, as the name suggests, every piece of data data is given a name. The piece of data that a name refers to is entirely arbitrary and could represent a frame of a YouTube video, a message in a chat room, or a command given to a smart home device. Similarly, the meaning behind the names are entirely arbitrary from the point of view of routers. They key aspect is that data can be requested from the network by name, removing the requirement of knowing \textit{where} the data is stored. NDN names consist of a set of "/" delimited values and the naming scheme used by an application is left up to the application developer. This provides flexibility to developers, allowing them to structure the names for their data in a way which makes sense to the application.

NDN exposes two core primitives - \textit{Interest} packets and \textit{Data} packets. In order to request a piece of data from the network, an Interest packet is sent out with the name field set to the name of the required piece of Data. For example, one might request the 100th frame of a video feed of a camera situated in a kitchen by expressing an Interest for the piece of data named \textit{/house/kitchen/videofeed/100} and this is done using the Interest primitive.

In the simplest case, the producer of the data under this name, the camera in the kitchen for example, will receive this request and can respond by sending the data encapsulated in a Data packet, with the name field set to the name of the interest.

NDN communication is entirely driven by consumers who request data by sending interests and any unexpected data packets which reach NDN nodes are simply ignored.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Packet Structure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NDN Packet Structures}\label{sec:ndn-packet-structure}
As outlined in the NDN Packet Specification \cite{ndn-packet-spec}, Interest and Data packets consist of required and optional fields. Optional fields which are not present are interpreted as a predetermined default value. The packet structure for both Interest and Data packets are shown in \reffig{fig:ndn-packet-structure}, where red fields represent required fields and blue fields represent optional fields.

\begin{figure}[H]
    \centering
    \figsize{assets/soa/ndn/packet-spec.png}{0.5}
    \caption{NDN Packet Structure \cite{ndn}}
    \label{fig:ndn-packet-structure}
\end{figure}

The fields contained in NDN Interest packets are outlined below.
\begin{labeling}{Interest Packets}
    \item [Name] The name of the content the packet refers to.
    \item [CanBePrefix] Indicates whether this Interest can be satisfied by a Data packet with a name such that the Interest packet's \textit{Name} field is a prefix of the Data packet's \textit{Name} field. This is useful when consumers do not know the exact name of a Data packet they require. If this field is omitted, the \textit{Name} of the Data packet must \textbf{exactly} match the \textit{Name} of the Interest packet. 
    \item [MustBeFresh] Indicates whether this Interest packet can be satisfied by a CS entry whose \textit{FreshnessPeriod} has expired.
    \item [ForwardingHint] This defines where the packet should be forwarded towards if there is no corresponding FIB entry. Due to limited capacity of the FIB, only a small number of name prefixes can be stored and the \textit{ForwardingHint} field can aid in mapping application data name prefixes to sets of globally “reachable” names \cite{ndn-dns}. This field typically represents an ISP prefix and is used to tackle the routing scalability issues present in NDN \cite{ndn-forwarding-hint}.
    \item [Nonce] A randomly generated 4-octet long byte string. The combination of the \textit{Name} and \textit{Nonce} should uniquely identify an Interest packet. This is used to detect looping Interests \cite{ndn-packet-spec}.
    \item [InterestLifetime] The length of time in milliseconds before the Interest packet times out. This is defined on a hop-by-hop basis, meaning that that an Interest packet will time out at an intermediate node \textit{InterestLifetime} milliseconds after reaching that node.
    \item [HopLimit] The maximum number of times the Interest may be forwarded.
    \item [Parameters] Arbitrary data to parameterize an Interest packet.
\end{labeling}

\vspace{5mm}
The fields contained in NDN Data packets are outlined below.

\begin{labeling}{Data Packets}
    \item [Name] The name of the content the packet refers to.
    \item [ContentType] Defines the type of the content in the packet. This field is an enumeration of four possible values: \textit{BLOB, LINK, KEY} or \textit{NACK}. \textit{LINK} and \textit{NACK} represent NDN implementation packets, while \textit{BLOB} and \textit{KEY} represent actual content packets and cryptographic keys respectively.
    \item [FreshnessPeriod] This represents the length of time in milliseconds that the Data packet should be considered fresh for. As Data packets are cached in the CS, this field is used to approximately specify how long this packet should be considered the newest content available for the given \textit{Name}. Consumers can use the \textit{MustBeFresh} field of the Interest packets to specify whether they will accept potentially stale cached copies of a piece of Data and the \textit{"staleness"} of the Data is defined using the \textit{FreshnessPeriod} field.
    \item [FinalBlockId] This is used to identify the ID of the final block of data which has been fragmented.
    \item [Content] This is an arbitrary sequence of bytes which contains the actual data being transported.
    \item [Signature] This contains the cryptographic signature of the Data packet
\end{labeling}

An important note to make is that neither the Interest nor Data packets contain any source or destination address information. This is a key component of NDN as it allows a single Data packet to be reused by multiple consumers. 

\subsection{NDN Basic Operation}\label{sec:ndn-basic-operation}
NDN requires three key data structures to operate - a \textit{Forwarding Information Base (FIB)}, a \textit{Pending Interest Table (PIT)} and a \textit{Content Store (CS)}. 

The FIB is used to determine which interface(s) an incoming Interest should be forwarded upstream through. This is similar to an FIB used on IP routers, however NDN supports multipath forwarding (see \refsec{sec:multipath-forwarding}), enabling a single Interest to be sent upstream through multiple interfaces. 

As discussed in \refsec{sec:routing}, NDN uses \textit{stateful forwarding} and the PIT is the data structure which maintains the forwarding state. This table stores the names of Interests and the interface on which the Interest was received, for Interests which have been forwarded upstream, but not yet had any Data returned.

Finally, the CS is used to cache data received in response to Interests expressed. The CS allows any NDN node to satisfy an interest if it has the corresponding Data pakcet, even if it is not the producer itself. As with all caches, the CS is subject to a replacement policy, which is typically \textit{Least Recently Used (LRU)}.

NDN also offers a \textit{Face} abstraction. An NDN Face is a link over which NDN Interest and Data packets can flow. A Face can represent a physical interface such as a network card, or a logical interface such as an application running on a machine producing data under a certain namespace.


The operation of an NDN node on receipt of an Interest packet is shown in \reffig{fig:ndn-on-interest}.  

\begin{figure}[H]
    \centering
    \figsize{assets/soa/ndn/incoming-interest.png}{0.5}
    \caption{NDN operation on receiving Interest}
    \label{fig:ndn-on-interest}
\end{figure}

On receiving an Interest, the CS is checked to see if there is a cached copy of the Data corresponding to the name in the Interest. If a copy exists with the appropriate freshness, the Data packet can simply sent back over the requesting Face and the Interest packet is satisfied. 

If there is no cached copy of the Data in the CS, the PIT is then checked. If a PIT entry containing the Interest name exists, this indicates that an equivalent Interest packet has already been requested and forwarded upstream. Thus, the Interest packet is \textbf{not forwarded upstream} a second time. Instead, the requesting Face is added to the list of downstream faces in the PIT entry. This list of faces represents the downstream links which are interested in a copy of the Data.

If there is no PIT entry, the FIB is then queried to extract the next hop information for the given Interest. If there is no next hop information, a NACK is typically returned. In some implementations, the Interest could also be forwarded based on the \textit{ForwardingHint} if one is present. If an FIB entry is present, a PIT entry for the given Interest is created and the packet is forwarded upstream. 

The operation of an NDN node on receipt of a Data packet is shown in \reffig{fig:ndn-on-data}.
\begin{figure}[H]
    \centering
    \figsize{assets/soa/ndn/incoming-data.png}{0.5}
    \caption{NDN operation on receiving Data}
    \label{fig:ndn-on-data}
\end{figure}


On receiving a Data packet, the PIT is checked to ensure that the Data packet had actually been requested. If there is no PIT entry, the node never expressed an Interest for this piece of Data. This means the Data is unsolicited and is typically dropped.

Otherwise, the Data packet is sent over all of the requesting faces contained in the PIT entry, the PIT entry is removed and the Data is added to the CS.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Names
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Names}\label{sec:ndn-names}
As with IP addresses, NDN names are hierarchical. This can be beneficial to applications as it allows for naming schemes which model relationships between pieces of data. In order to support retrieval of dynamically generated data, NDN names must be deterministically constructable. This means there must be a mechanism or agreed upon convention between a data producer and consumer to allow consumers to fetch data \cite{ndn-project}. 

The names of Data packets can be more specific than the names of the Interest packets which trigger them. That is, the Interest name may be a prefix of the returned Data name. For example, a producer of sequenced data may respond to Interests of the form \textit{/ndn/test/<sequence-number>}.  In this case, the producer would register the prefix \textit{/com/test} in order to receive all Interests, regardless of the sequence number requested. However a consumer may not know what the current sequence number is. Thus a convention could be agreed upon such that a consumer can express an interest for \textit{/com/test} and the Data packet that will be returned will be named \textit{/com/test/<next-producer-sequence-number>}, allowing the consumer to catch up to the current sequence number. This method is used in the synchronization protocol outlined in \todo[inline]{ref sync protocol}.














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Routing and Forwarding
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Routing and Forwarding}\label{sec:routing}
Longest prefix, hierarchical naming (ndn project has some stuff on p3 2.2.1 names), NLSR, stateful forwarding can allow routers to measure performance of routes and update things accordingly (they see packets going out AND COMING BACK unlike Implementations, also it is what enables multicast and in network caching)

IP routers use \textit{stateful routing} and a \textit{stateless forwarding plane}. This means that routers maintain some state on where to forward packets given their destination IP addresses (stateful routing), but when it comes to actually forwarding packets, the packets are sent over the chosen route and forgotten about (stateless forwarding). NDN on the other hand, uses both stateful forwarding and routing \cite{stateful-forwarding} in order to accomplish routing packets by name and not address, as seen by the usage of a PIT. 

As discussed in \refsec{sec:ndn-names}, NDN names are hierarchical. This allows NDN routing to scale, in a similar manner to how routing scales by exploiting the hierarchical nature of IP addresses \cite{stateful-forwarding}.

The use of a stateful forwarding plane is NDN has some drawbacks such as added router operation complexity and the addition of a new attack vector through router state exhaustion attacks, due to the limited size of the PIT \cite{case-against-stateful-forwarding}. However, there are three key benefits offered by NDN's stateful forwarding plane - multipath forwarding, native multicast and adaptive forwarding.

\sneaktitle{Multipath Forwarding} \label{sec:multipath-forwarding}
One of the challenges of routing IP packets using a stateless forwarding plane is ensuring that there are no forwarding loops. Otherwise a single packet could loop endlessly throughout the network. The typical approach to solving this problem is to use the \textit{Spanning Tree Protocol (STP)} \cite{spanning-tree-protocol} to build a loop free topology. This results in a single optimal path between any two nodes in a network and disables all other paths.

However, as NDN uses a stateful forwarding plane, Interest packets cannot loop. As discussed in \refsec{sec:ndn-packet-structure}, Interest's contain a \textit{Nonce} field, allowing Interests to be uniquely identified. If an NDN router sees an Interest which is identical to an Interest in the PIT, the Interest is ignored as a loop has been detected. That is, the usage of a PIT prevents looping. Similarly, as Data packets take the reverse path of the Interest packets, they also cannot loop. 

This means NDN can natively support multipath forwarding. This is done by allowing multiple next hops for a given entry in the FIB. This provides flexibility in the routing protocols which can be used with NDN and offers several benefits such as load balancing across entries in the FIB. Thus, to take advantage of the native multipath forwarding capabilities, a NDN specific routing protocol was developed (see \refsec{sec:NLSR})

\sneaktitle{Native Multicast}
As discussed in \refsec{sec:ndn-basic-operation}, when a router receives an Interest which matches an entry in the PIT, it does not forward the second Interest upstream. Instead it adds the Face over which the incoming Interest was received to the PIT entry. Once the data for the Interest reaches the router, it forwards the Data packet to \textbf{all} of the faces listed in the PIT entry. Thus, NDN natively supports multicast as a producer may produce a single Data packet and have it reach many consumers. 

\sneaktitle{Adaptive Forwarding}
As NDN's forwarding plane is stateful, routers can dynamically adapt where they forward packets as the needs arise. Routers can track performance metrics such as round-trip-times of upstream connections and can use this information to detect temporary link failures, or poorly performing links and route around them.

\subsection{In-Network Storage}
As discussed in \refsec{sec:ndn-basic-operation}, a Data packet is entirely independent of who requested it or where it was obtained from, allowing a single Data packet to be reused for multiple consumers \cite{ndn}. The CS of routers provides a mechanism for opurtunistic in-network caching, which can help reduce traffic load for popular content. 

NDN also supports larger volume, persistent in-network storage in the form of \textit{repo-ng} \cite{ndn-repo}, which supports typical remote dataset operations such as reading, inserting into and deleting data objects \cite{ndn-repo-homepage}. This mechanism provides native network level support for Content Delivery Networks (CDN) \cite{ndn} and can allow applications to go offline for longer periods of 
time while their content is served from in-network repositories.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Forwarding Strategies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Forwarding Strategies}
The choice of how to forward packets in NDN is defined by a \textit{forwarding strategy}. Several strategies have been designed for NDN such as \textit{Best Route}, \textit{NCC}, \textit{Multicast} and \textit{Client Control} \cite{ndn-sim-forwarding-strategies}. However, \textit{Best Route} and \textit{Multicast} are the most common. To forward packets, a list of possible next-hops is obtained from the FIB for a given Interest. For the \textit{Best Route} strategy, the Interest is forwarded over the best performing Face, ranked by a certain metric such as link cost or round trip time. For the \textit{Multicast} strategy, Interests are forwarded over all Faces which are obtained from the FIB for a given Interest.

As one would expect, Forwarding strategies play a major role in the performance of an application using NDN. For example the \textit{Multicast} strategy should be used only in scenarios where multicast is beneficial or required as it can cause a major increase in the number of Interests which must be sent across the network. However application’s correctness can also be affected by the forwarding strategy \cite{forwarding-strategies}. For example, if a \textit{Best Route} strategy is used in a distributed dataset synchronization context, it is possible that only a subset of participants will see published updates and thus \textit{Multicast} should be used in this context. This is outlined further in \todo[]{Reference Player Discovery multicast}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Security
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Security}
The aspect of security in NDN is the shift from attempting to secure communication channels to focusing on securing the data itself at production time. 

Typically, the main form of secure communication in the Internet today is using the Transport Layer Security (TLS) protocol \cite{TLS}  along with the Transmission Control Protocol (TCP) over IP (TCP/IP). As discussed in \refsec{sec:ndn-sota}, TCP/IP is a mechanism for allowing communication between two nodes in a network. TCP/IP sets up a communication channel between the hosts and TLS is used to secure that channel. 

NDN on the other hand focuses on securing the Data packets produced in response to Interests. As shown in \refsec{sec:ndn-packet-structure}, NDN Data packets must contain a \textit{Signature} field. A cryptographic signature is generated using the producers public key, binding the producer's name to the content. \cite{ndn-security-overview}.

As NDN uses public key cryptography, all applications and nodes must thus have their own set of keys and a means for determining which keys can legitimately sign which pieces of data. NDN uses three key components in this regard - \textit{NDN Certificates}, \textit{Trust Anchors} and \textit{Trust Policies}.

NDN Certificates bind a user's name to its key and certifies the ownership of this key \cite{ndn-security-overview}. Trust Anchors are the certificate authority for a given NDN namespace. NDN nodes can then verify published certificates by backtracking along the trust chain until a Trust Anchor is reaced. Finally, Trust Policies are used by applications to define whether or not they will accept certain packets based on naming rules and Trust Anchors.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NFD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NFD}
In order to provide the NDN functionality, the \textit{NDN Forwarding Daemon (NFD)} was developed. NFD is a network forwarder that implements the NDN protocol \cite{nfd-github}. The NFD thus implements all of the features described in \refsec{sec:ndn-basic-operation} such as the CS, PIT and FIB. 

 As NDN strives to replace IP as the universal network layer, NDN can run over a variety of lower level protocols such as Ethernet, TCP/IP and UDP/IP. The NFD provides this functionality by abstracting communication to dealing with \textit{Faces}. \textit{Faces} can be backed by a variety of transport mechanisms such as UDP/TCP tunnels or Unix sockets. This allows applications using the NDN Common Client Libraries (see \refsec{sec:ndn-tools}) to communicate with the NFD through the Face abstraction.

 The API of the NFD provides a means for creating \textit{Faces}, adding \textit{Routes} and specifying \textit{Forwarding Strategies}.
 
 A typical set up of applications using the NFD is shown in \reffig{fig:nfd-setup}. The NFD requires faces to be created before operation. In this case, as part of the setup procedure, node A would create a \textit{Face} towards node B, backed by a UDP tunnel towards node B's IP address. Node A would also create a \textit{Route} towards node B by specifying the prefix node B is responsible for, along with the ID of the Face previously created. In this case the route would map \textit{/ndn/nodeB} to face 2. Note that this process is somewhat automated by using the prefix discovery protocol of NLSR (see \refsec{sec:NLSR}). Finally, node A can specify the \textit{Forwarding Strategy}, or use the default of \textit{Best Route}.


As nodeB is a producer, it only needs to create a \textit{Face} towards the local NFD (face 7 in this case) and inform the NFD that it will be producing data under the prefix \textit{/ndn/nodeB}. This is done using the  \textit{registerPrefix} call provided by the NDN client library. This will create the a route in node B's NFD which maps \textit{/ndn/nodeB} to face 7.
\begin{figure}[H]
    \centering
    \figsize{assets/soa/ndn/nfd.png}{1}
    \caption{An example NFD setup}
    \label{fig:nfd-setup}
\end{figure}

With the NFDs configured, an example operation would be the following (with some of the basic operations of NDN omitted for brevity):

\begin{enumerate}
    \item Node A's application creates a face towards the local NFD (face 5 in this case).
    \item Node A requests node B's status by expressing an Interest for \textit{/ndn/nodeB/status} through face 5.
    \item Node A's NFD checks the CS and PIT which are empty and finally determines the next-hop for the Interest is through face 2.
    \item Node A's NFD sends the Interest through the UDP tunnel towards node B's NFD which accepts UDP connections on NFD's default port of 6363. This creates face 9 on node B's NFD in the process.
    \item Node B's NFD then finds the FIB entry created for \textit{/ndn/nodeB} when nodeB registered the prefix and forwards the Interest over face 7
    \item Node B's application will then create the corresponding Data packet and send it over face 7
    \item Node B's NFD will check the PIT for a list of faces to forward this Data packet over and will find face 9
    \item The Data packet will reach Node A's NFD via the UDP tunnel
    \item Node A's NFD will extract the list of downstream faces for this Data packet from the PIT and will send the packet over face 5 to node A's application.
\end{enumerate}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NLSR
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NLSR}\label{sec:NLSR}\todo[]{This section isn't great}
In order to facilitate router and prefix discovery, the \textit{NDN Link State Routing (NLSR)} protocol was developed. As the name suggests, NLSR is a \textit{Link State Routing (LSR)} protocol \cite{lsr-rfc}. 

The LSR protocol models the network as a directed, weighted graph in which each router is a node. The main purpose of LSR is to discover the network topology, allowing routers to compute routing tables using a shortest path algorithm such as Djikstra's Algorithm \cite{djikstra}\cite{lsr}. To do this, LSR routers need a mechanism for discovering adjacent routers. However, as this is the process used to \textit{build} routing tables, it cannot make use of existing routing tables. Thus, LSR periodically broadcasts \textit{HELLO} messages over all of the router's interfaces. These messages contain the router's unique address, allowing routers to discover their immediately adjacent neighbours. 

The routers then need to reliably disseminate the list of their adjacent neighbours to all other routers in the network, so that all routers have a full view of the network topology. This is done using \textit{Link State Packets (LSPs)}. LSPs contain the list of direct neighbours for a given router and the edge weight (link cost) for each of those neighbour connections. Unlike the \textit{HELLO} messages for neighbour discovery, routers will forward LSPs from a specific router to their direct neighbours, \textbf{once per sequence number}. Thus, routers need to maintain state containing the most recent LSP it has seen for each router in order to determine whether or not a given LSP is newer than what it has already seen and thus whether or not to forward this version. This information is maintained inside the \textit{Link State Database (LSDB)}. This process is known as the \textit{Flooding algorithm} and allows all nodes to discover the full network topology and to build their routing tables accordingly.

NLSR is designed as an \textbf{intra domain} routing protocol. As it is to be used for NDN, it is imperative that it operates solely using NDN's primitives (see \refsec{sec:ndn-basic-operation}). Thus it uses Interest and Data packets as the only form of communication between routers. NLSR differs from the traditional IP based LSR protocol in the following ways as it uses hierarchical naming schemes for routers, keys and updates, it uses a hierarchical trust model, it uses ChronoSync to disseminate routing updates (see \refsec{sec:chronosync}) and it supports multipath routing \cite{nlsr}. 

\begin{figure}[H]
    \centering
    \figsize{assets/soa/ndn/LSAs.png}{0.7}
    \caption{NLSR LSA structure \cite{nlsr} (adapted)}
    \label{fig:nlsr-lsa}
\end{figure}

As seen in \reffig{fig:nlsr-lsa}, NLSR uses \textit{Link State Advertisements (LSAs)} which can be one of two types - \textit{name} or \textit{adjacency}. \textit{Name} LSAs contain the list of prefixes which this router may produce data for, while \textit{adjacency} LSAs contain the list of neighbours a router has as well as their associated link costs. LSA dissemination is essentially a dataset synchronization problem and thus NLSR uses NDN's ChronoSync protocol (see \refsec{sec:chronosync}) to synchronize the LSAs. \textit{Name} LSAs can be updated as registered prefixes change, while \textit{adjacency} LSAs can change as routers go offline and come back online. In the steady state, all routers will maintain an outstanding sync Interest, containing the same digest of the LSA dataset. This outstanding sync Interest will be named \textit{/<network>/nlsr/sync/<digest>} and the forwarding strategy of \textit{/localhop/com/nlsr/sync/} is set to multicast on all routers, allowing all routers to receive sync updates. If an LSA is changed on a particular router, the router responds to the outstanding sync Interest with a Data packet containing the \textbf{name} of the next version if the LSA. Other routers can then fetch this updated LSA using a standard Interest packet when convenient.  

As with LSR, NLSR is responsible for building the FIB and thus requires a mechanism to discover adjacent routers. This is accomplished by setting the forwarding strategy of \textit{<network>/nlsr/LSA} to multicast. When a new router receives the first response to the sync Interest it expresses, it can request the Data using the corresponding name and this Interest will be multicasted to all of the router's adjacent neighbours. This is required as the router's FIB may not have an entry for the corresponding name. However, this broadcast should not have any extra overhead as Interests will be aggregated by intermediate routers and every router in the network would need to be informed of the LSA change. Thus, Interest Aggregation at intermediate routers means NLSR is more efficient at disseminating routing updates than the corresponding \textit{Flooding} algorithm in IP.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Tools
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NDN Tools and Libraries}\label{sec:ndn-tools}
In order to facilitate the development of NDN applications, the API specified in the NDN Common Client Libraries Documentation \cite{ndn-ccl} has been ported to a variety of popular languages such as C++, Python and Java. Several command line tools under the NDN Tools project \cite{ndn-tools} have also been developed which provide useful NDN functionality such as pinging remote NFDs, expressing interests, analysing packets on the wire and segmented file transfer.

An NDN simulation tool called ndnSIM \cite{ndn-sim-webpage} has also been developed to facilitate experimentation using the NDN architecture. This project has been under continuous development since 2012 and has been used by hundreds of researchers around the world \cite{ndnsim}.  

Another project built by the NDN team is Mini-NDN \cite{mini-ndn}. Mini-NDN is based on the popular network emulation tool Mininet \cite{mininet} and allows for the emulation of a full NDN network on a single system. This provides a convenient way to get up and running with NDN and to test NDN applications.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NDN Benefits in MOG
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{NDN Benefits in a MOG Context}
Interest aggregation, in network caching, native multicast, multipath forwarding.

As outlined in \refsec{sec:mogs}, MOGs can be built using a variety of architectures, can have a variety of different data types and require extremely performant networking solutions. As such, MOGs are an excellent test of the performance of new networking technologies and architectures such as NDN. The three major benefits offered by NDN in a MOG context are \textit{Interest aggregation}, \textit{native multicast} and \textit{in-network caching}.

\sneaktitle{Interest Aggregation}
As discussed in \refsec{sec:ndn-basic-operation}, the use of a PIT allows NDN routers to aggregate Interests and has the potential to drastically reduce network traffic in the process. In a P2P MOG architecture, every player is typically interested in the data being produced by every other player. This means there are $n^2$ logical connections required where $n$ is the total number of game players, assuming the typical architecture in which every player is responsible for publishing their own updates. In a traditional UDP/IP based implementation, all $n^2$ of these logical connections are required as actual connections via a UDP tunnels, or something similar.

However, in an NDN based implementation, considering \textit{"connections"} no longer makes sense. Considering the fact that $n-1$ players are likely to be expressing Interests for a given player's Data, Interest aggregation plays a major role in reducing network traffic, as only one instance of the same Interest will be forwarded upstream by each intermediate router. Thus, as the Interests from separate consumers are forwarded closer and closer to the producer, it is more likely that they will reach a common intermediate router and be aggregated, although this depends on the topology. The earlier this occurs in the topology the better, as it counteracts the issue stemming from the $n^2$ logical connections required due to the P2P architecture. Interest aggregation would also benefit Client/Server architecture in much the same way, as Interests would be aggregated on route to the server just as they would be in a P2P architecture while on route to a producer.

This also provides a benefit from the point of view of game players as publishers, as they should only see and need to respond to \textbf{one instance} of each Interest, provided consumers only request Data within the freshness period, as Interests will be aggregated at their local NFDs as well. An example of Interest aggregation is seen at $time = t2$ of \reffig{fig:agg-multicast}.

\begin{figure}[H]
    \centering
    \figsize{assets/soa/ndn/agg-multicast.png}{0.9}
    \caption{Interest aggregation, native multicast and in-network caching}
    \label{fig:agg-multicast}
\end{figure}


\sneaktitle{Native Multicast}
The core concept behind multicast is producing a piece of data once and having it reach multiple consumers and NDN provides this natively. Native multicast is a direct result of NDN's stateful forwarding plane, Interest aggregation mechanism and in-network caching. Considering MOG networking from a higher level, the architecture is essentially one of \textit{publish-subscribe (pub-sub)}, in which game players (publishers) must publish data to all other players in the game (subscribers). Native multicast is a direct benefit over traditional UDP/IP which requires the same piece of data to be sent to every client, requiring $\mathcal{O}(n)$ sends. An example of native multicast is seen at $time = t3$ of \reffig{fig:agg-multicast}.

\sneaktitle{In-Network Caching}
As NDN routers use opportunistic caching via the CS, frequently requested, or recently produced Data packets can be cached and served by intermediate routers, reducing the round-trip-times of fetching updates from the network and thus the overall latency of the MOG. Although static content (see \refsec{sec:taxonomy}) would likely see relatively high cache rates, the frequency at which static content would be fetched would likely be as low as once per game, meaning the overall network impact would likely be negligible in comparison to the more frequently fetched data. 

However, considering the outstanding Interest architecture in which all consumers keep an outstanding Interest and wait for producers to produce the next Data packet (see \todo[]{custom sync protocol ref}, the effects of caching come into play in the case where a consumer falls slightly behind in fetching remote updates. For example, if a publisher produces a Data packet every 100ms, it is likely that, in the steady-state, Interests from most of the consumers would be aggregated while the consumers wait on the next packet to be produced. Once this packet is produced, the Data will be multicasted back to all consumers who requested it as previously described. However, if a consumer falls slightly behind other consumers and expresses an Interest for a piece of Data which has already been produced, without caching, this would require a full round trip all the way to the producer. This would likely occur concurrently to when other consumers are requesting the \textbf{next} Data packet, meaning the consumer will continue to remain behind and continue to essentially \textbf{double} the number of Interests seen by the producer and largely increase the amount of network traffic required for a certain sequenced piece of Data.  

However, if caching is used, this Data can be returned from the CS of the first intermediate router who previously forwarded this Interest on behalf of another consumer. Thus, the consumer can potentially receive this somewhat stale Data much quicker and will hopefully catch up with the other consumers, or continue to obtain cached copies previously fetched. An example of in-network caching is shown in \reffig{fig:agg-multicast} at $time = t4$.

\section{Real Time Applications using NDN}

\section{Distributed Dataset Syncrhonization}
Summarize the survey paper
\subsection{ChronoSync}\label{sec:chronosync}
Limitations of chronosync / others
\subsection{PSync}
\subsection{CCNx Sync}

\section{Mutliplayer Online Games (MOGs)}\label{sec:mogs}
\subsection{How Modern MOGs are Built}
Ashley / libgdx etc.

\subsection{Taxonomy of data}\label{sec:taxonomy}
\begin{figure}[H]
    \centering
    \figsize{assets/soa/mog/taxonomy-with-game-data.png}{0.9}
    \caption{Taxonomy of MOG Data}
    \label{fig:taxonomy}
\end{figure}

\subsection{Architectures (C/S, P2P, Hybrid)}
This is particularly evident in applications built on a peer-to-peer architecture, where a lack of a single authority presents additional challenges in synchronising the state of shared objects while presenting a responsive simulation \cite{dead-reckoning}


\subsection{Dead Reckoning}
Reconciliation between dead reckoned position and remote update methods. Dead reckoning (DR) is a short-term linear extrapolation algorithm which utilises information relating to the dynamics of an entity’s state and motion, such as position and velocity, to model and predict future behaviour. \cite{dead-reckoning}


\subsection{Area of Interest Management}
PubSub paper has interesting stuff on IZF
\begin{figure}[H]
    \centering
    \figsize{assets/soa/mog/tile-interest-zone.png}{0.6}
    \caption{Tile-based area of interest management}
    \label{fig:tile-izm}
\end{figure}

\subsection{Lag Compensation}
Read up on lag compensation etc, Source Engine notes

\section{Closely Related Projects}
\sneaktitle{Egal Car \cite{egal-car}}
\sneaktitle{Matryoshka \cite{ndn-multiplayer-game}}
\sneaktitle{NDNGame \cite{ndn-game}}

